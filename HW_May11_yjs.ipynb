{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"HW_May11_yjs.ipynb","provenance":[],"authorship_tag":"ABX9TyNmgyIaLVDhC3rjezdTwR4x"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"8ZnDWZOqw6Jl","colab_type":"code","colab":{}},"source":["def policy_eval(policy, env, discount_factor=1.0, theta=0.00001):\n","  # Start with a random (all 0) value function\n","  V = np.zeros(env.nS)\n","  while True: \n","    delta = 0\n","    # For each state, perform a \"full backup\"\n","    for s in range(env.nS):\n","      v = 0 \n","      # Look at the possible next actions\n","      for a, action_prob in enumerate(policy[s]):\n","        # For each action, look at the possible next states...\n","        for prob, next_state, reward, done in env.P[s][a]:\n","          # Calculate the expected value\n","          v += action_prob * prob * (reward + discount_factor * V[next_state])\n","      # How much our value function changed (across any states)\n","      delta = max(delta, np.abs(v - V[s]))\n","      V[s] = v\n","    # Stop evaluating once our value function change is below a threshold\n","    if delta < theta:\n","      break\n","  return np.array(V)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-ZdMN_9g7wSs","colab_type":"code","colab":{}},"source":["#@title\n","import io\n","import numpy as np\n","import sys\n","from gym.envs.toy_text import discrete\n","\n","UP = 0\n","RIGHT = 1\n","DOWN = 2\n","LEFT = 3\n","\n","class GridworldEnv(discrete.DiscreteEnv):\n","    \"\"\"\n","    Grid World environment from Sutton's Reinforcement Learning book chapter 4.\n","    You are an agent on an MxN grid and your goal is to reach the terminal\n","    state at the top left or the bottom right corner.\n","    For example, a 4x4 grid looks as follows:\n","    T  o  o  o\n","    o  x  o  o\n","    o  o  o  o\n","    o  o  o  T\n","    x is your position and T are the two terminal states.\n","    You can take actions in each direction (UP=0, RIGHT=1, DOWN=2, LEFT=3).\n","    Actions going off the edge leave you in your current state.\n","    You receive a reward of -1 at each step until you reach a terminal state.\n","    \"\"\"\n","\n","    metadata = {'render.modes': ['human', 'ansi']}\n","\n","    def __init__(self, shape=[4,4]):\n","        if not isinstance(shape, (list, tuple)) or not len(shape) == 2:\n","            raise ValueError('shape argument must be a list/tuple of length 2')\n","\n","        self.shape = shape\n","\n","        nS = np.prod(shape)\n","        nA = 4\n","\n","        MAX_Y = shape[0]\n","        MAX_X = shape[1]\n","\n","        P = {}\n","        grid = np.arange(nS).reshape(shape)\n","        it = np.nditer(grid, flags=['multi_index'])\n","\n","        while not it.finished:\n","            s = it.iterindex\n","            y, x = it.multi_index\n","\n","            # P[s][a] = (prob, next_state, reward, is_done)\n","            P[s] = {a : [] for a in range(nA)}\n","\n","            is_done = lambda s: s == 0 or s == (nS - 1)\n","            reward = 0.0 if is_done(s) else -1.0\n","\n","            # We're stuck in a terminal state\n","            if is_done(s):\n","                P[s][UP] = [(1.0, s, reward, True)]\n","                P[s][RIGHT] = [(1.0, s, reward, True)]\n","                P[s][DOWN] = [(1.0, s, reward, True)]\n","                P[s][LEFT] = [(1.0, s, reward, True)]\n","            # Not a terminal state\n","            else:\n","                ns_up = s if y == 0 else s - MAX_X\n","                ns_right = s if x == (MAX_X - 1) else s + 1\n","                ns_down = s if y == (MAX_Y - 1) else s + MAX_X\n","                ns_left = s if x == 0 else s - 1\n","                P[s][UP] = [(1.0, ns_up, reward, is_done(ns_up))]\n","                P[s][RIGHT] = [(1.0, ns_right, reward, is_done(ns_right))]\n","                P[s][DOWN] = [(1.0, ns_down, reward, is_done(ns_down))]\n","                P[s][LEFT] = [(1.0, ns_left, reward, is_done(ns_left))]\n","\n","            it.iternext()\n","\n","        # Initial state distribution is uniform\n","        isd = np.ones(nS) / nS\n","\n","        # We expose the model of the environment for educational purposes\n","        # This should not be used in any model-free learning algorithm\n","        self.P = P\n","\n","        super(GridworldEnv, self).__init__(nS, nA, P, isd)\n","\n","    def _render(self, mode='human', close=False):\n","        \"\"\" Renders the current gridworld layout\n","         For example, a 4x4 grid with the mode=\"human\" looks like:\n","            T  o  o  o\n","            o  x  o  o\n","            o  o  o  o\n","            o  o  o  T\n","        where x is your position and T are the two terminal states.\n","        \"\"\"\n","        if close:\n","            return\n","\n","        outfile = io.StringIO() if mode == 'ansi' else sys.stdout\n","\n","        grid = np.arange(self.nS).reshape(self.shape)\n","        it = np.nditer(grid, flags=['multi_index'])\n","        while not it.finished:\n","            s = it.iterindex\n","            y, x = it.multi_index\n","\n","            if self.s == s:\n","                output = \" x \"\n","            elif s == 0 or s == self.nS - 1:\n","                output = \" T \"\n","            else:\n","                output = \" o \"\n","\n","            if x == 0:\n","                output = output.lstrip()\n","            if x == self.shape[1] - 1:\n","                output = output.rstrip()\n","\n","            outfile.write(output)\n","\n","            if x == self.shape[1] - 1:\n","                outfile.write(\"\\n\")\n","\n","            it.iternext()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mH0jBH0Vz6JS","colab_type":"code","colab":{}},"source":["def policy_improvement(env, policy_eval_fn = policy_eval, discount_factor = 1.0):\n","  def one_step_lookahead(state, V):\n","    A = np.zeros(env.nA)\n","    for a in range(env.nA):\n","      for prob, next_state, reward, done in env.P[state][a]:\n","          A[a] += prob * (reward + discount_factor * V[next_state])\n","    return A \n","\n","  # Start with a random policy\n","  policy = np.ones([env.nS, env.nA]) / env.nA\n","\n","  while True: \n","    # Evaluate the current policy\n","    V = policy_eval_fn(policy, env, discount_factor) # fn????????\n","\n","    # Will be set to false if we make any changes to the policy\n","    policy_stable = True\n","\n","    # For each state...\n","    for s in range(env.nS):\n","      # The best action we would take under the current policy\n","      chosen_a = np.argmax(policy[s])\n","\n","      # Find the best action by one-step lookahead\n","      # Ties are resolved arbitrarily\n","      action_values = one_step_lookahead(s, V)\n","      best_a = np.argmax(action_values)\n","\n","      # Greedily update the policy\n","      if chosen_a != best_a:\n","        policy_stable = False\n","      policy[s] = np.eye(env.nA)[best_a]\n","\n","    # If the policy is stable we've found an optimal policy. Return it\n","    if policy_stable:\n","      return policy, V\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SdEl8vx07m6b","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":578},"outputId":"96c5cd95-26b8-467b-fed4-e0604bae407f","executionInfo":{"status":"ok","timestamp":1589170362796,"user_tz":-540,"elapsed":560,"user":{"displayName":"Yoo Jungsun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjvnWwk3NIFmjqCA1XSi68LDhJEc7WuXQMtPPylXQ=s64","userId":"08265690843164266334"}}},"source":["env = GridworldEnv()\n","policy, v = policy_improvement(env)\n","print(\"정책 확률 분포:\")\n","print(policy)\n","print(\"\")\n","\n","print(\"형태를 변형한 정책 그리드 (0=위, 1=오른쪽, 2=아래, 3=왼쪽):\")\n","print(np.reshape(np.argmax(policy, axis=1), env.shape))\n","print(\"\")\n","\n","print(\"가치 함수:\")\n","print(v)\n","print(\"\")\n","\n","print(\"형태를 변형한 가치 함수 그리드:\")\n","print(v.reshape(env.shape))\n","print(\"\")"],"execution_count":6,"outputs":[{"output_type":"stream","text":["정책 확률 분포:\n","[[1. 0. 0. 0.]\n"," [0. 0. 0. 1.]\n"," [0. 0. 0. 1.]\n"," [0. 0. 1. 0.]\n"," [1. 0. 0. 0.]\n"," [1. 0. 0. 0.]\n"," [1. 0. 0. 0.]\n"," [0. 0. 1. 0.]\n"," [1. 0. 0. 0.]\n"," [1. 0. 0. 0.]\n"," [0. 1. 0. 0.]\n"," [0. 0. 1. 0.]\n"," [1. 0. 0. 0.]\n"," [0. 1. 0. 0.]\n"," [0. 1. 0. 0.]\n"," [1. 0. 0. 0.]]\n","\n","형태를 변형한 정책 그리드 (0=위, 1=오른쪽, 2=아래, 3=왼쪽):\n","[[0 3 3 2]\n"," [0 0 0 2]\n"," [0 0 1 2]\n"," [0 1 1 0]]\n","\n","가치 함수:\n","[ 0. -1. -2. -3. -1. -2. -3. -2. -2. -3. -2. -1. -3. -2. -1.  0.]\n","\n","형태를 변형한 가치 함수 그리드:\n","[[ 0. -1. -2. -3.]\n"," [-1. -2. -3. -2.]\n"," [-2. -3. -2. -1.]\n"," [-3. -2. -1.  0.]]\n","\n"],"name":"stdout"}]}]}